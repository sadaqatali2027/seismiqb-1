{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State of the seismic interpretation\n",
    "\n",
    "This notebooks sums up our current progress in multiple seismic-related tasks:\n",
    "\n",
    "* [Horizon detection](detection)\n",
    "* [Horizon extension](extension)\n",
    "* [Interlayers segmentation](segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='detection'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizon detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task is closely connected with day-to-day work of seismic experts: the very first thing they need to work with the cube is to pick seismic horizons. Seismic horizon is a change in rock properties across a boundary between two layers of rock, particularly seismic velocity and density. Such changes are visible in seismic images (even for an untrained eye), and could be automatically detected. We have a [notebook](./Horizons_detection.ipynb) that demonstrates entire process of solving the task: from indexing dataset and data-feeding procedure to training neural network and computing the metrics. Some of the most important parts:\n",
    "\n",
    "* **Dataset:** we have 4 seismic cubes, each with 4-10 hand-labeled horizons. The smallest one is held out for testing purposes\n",
    "\n",
    "* **Data-feeding procedure:** due to the fact that we usually can't feed entire cube into GPU (only the smallest ones), we cut them into crops and use it as training data. We also use some augmentations like scaling, cutout, additive noise, elastic transform etc\n",
    "\n",
    "* **Neural network architecture:** `EncoderDecoder` is used; we use sophisticated building blocks (like `Inception` ones) while reducing spatial resolution in order to obtain information about inner structure of the crop, and then restore initial resolution to produce fine-grained labels\n",
    "\n",
    "* **Training procedure:** we use `Adam` optimizer for 1000 epochs to minimize `Dice` coefficient; 80% of ilines in each of the three cubes are used at train time\n",
    "\n",
    "* **Validation:** we first validate our model on the remaining 20% of the ilines in the three cubes; then we check model performance on the held-out cube in order to fairly evaluate model quality\n",
    "\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "We use multiple metrics: area of detected horizon (compared to the hand-labeled one), mean difference between horizons, area of detected horizon that is closer than 5ms to the ground truth. In order to provide exhaustive research, we also trained multiple models on different datasets.\n",
    "\n",
    "First of all, we created individual models for each of the three cubes. Moreover, we used only each 200-th iline, totalling in no more than 15 slides. Due to that fact that seismic cubes change slowly along its spatial dimensions, we expect our model to easily recover the rest of horizons. For conveniency, we created `Python` [scripts](./../scripts), that essentially repeat described pipeline but easier to mass-usage. Predictions from each model, tested on the same cube, scored following results:\n",
    "\n",
    "| Train/test cube |   Area, % | Mean error, ms | Area in 5ms window, % |\n",
    "| :------ | ----: | ----: | ----: |\n",
    "| CUBE_1 | 74, 74, 71  | 1.5, 2.1, 4.4 | 96, 94, 85 |\n",
    "| CUBE_3 | 92, 92, 92  | 1.8, 1.9, 2.5 | 99.8, 99.5, 91.6 | \n",
    "| CUBE_VU_ONGMK |  73, 83, 84 | 3, 3.6, 4.3 | 89, 86, 84 |\n",
    "\n",
    "Results in the table are consistent with our observations: detected horizonts are detected well, and the main point of improvement lies in enlarging the covered area. The only thing that catches the eye is suspiciously low area of detected horizons on `Cube_1`. Returning to the dataset [description](./../datasets/Horizonts_modelling.ipynb), we can easily identify the roots of the problem: hand-labeled horizons are for some reasons present in the zero-traces, and that skews our results.\n",
    "\n",
    "Models, trained only on one cube, hardly works on the others: it just can'd adjust to the altering of values in the traces. Thus, we need to train model on at least multiple cubes in order to generalize on the others. \n",
    "\n",
    "Having 3 cubes in total, we trained models for each pair of them and used it as predictor on the remaining one: results (with the same metrics as before) are shown in the table:\n",
    "\n",
    "| NOT in the training |   Area, % | Mean error, ms | Area in 5ms window, % |\n",
    "| :------ | ----: | ----: | ----: |\n",
    "| CUBE_1 | 5  | 0.85 | 94 |\n",
    "| CUBE_3 | 45  | 1 | 97 | \n",
    "| CUBE_VU_ONGMK |  25 | 50 | 0 |\n",
    "\n",
    "Again, results are not surprising: where test-cube structure resembles the train ones, model can follow the horizon quite well, with the easiest cube being labeled the best. It is the covered area that is the problem: different types of inner noises does not allow model to generalize well enough on big distances. The hardest cube (`Cube_VU_ONGMK`) stays pretty much unlabeled due to its unique hardness.\n",
    "\n",
    "### Criticism\n",
    "\n",
    "The task in hand is ill-defined: it is very unclear, which horizons on the slide we want to get, how many of them, with which rules of picking. Current labels are quite inconsistent: they have different phase (some of them are on maximum values of amplitude, some of them at minimums); they separate different objects: most of them are following the brightest line on the slide, some of them are between crucial seismic facies, few of them track fissures. Most of them are made by automatic autocorellation and not really interesting (nor hard).\n",
    "\n",
    "Despite that, this model serves as a great trampoline for the others: [horizont extension](./Horizons_extension.ipynb) and [facies segmentation](./Segmenting_interlayers.ipynb). Clear visual interpretation allows to compare different neural network architectures and test all of the methods of our library.\n",
    "\n",
    "### Suggestions for improvements\n",
    "\n",
    "It is easily seen that in this task the bottleneck is data: more diverse seismic cubes with more horizons would greatly improve both quality of the detected ones and allow to better generalize on completely unseen data. Nevertheless, we can use this type of models even in its current state to generate more data for other tasks: namely, [horizont extension](./Horizons_extension.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extension'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizon extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='segmentation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interlayers segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
